## Pretrain Models
预训练模型的优化和进展，包括但不仅限于BERT系列

### 论文
- [5t_exploring the limits of transfer learning](https://arxiv.org/abs/1910.10683)
- [attention is all you need](https://arxiv.org/abs/1706.03762)
- [BERT pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)
- [ELECTRA Pre training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)
- [k-BERT enabling language representation with knowledge graph](https://arxiv.org/abs/1909.07606)
- [Low-Rank Bottleneck in Multi-head Attention Models](https://arxiv.org/abs/2002.07028)
- [Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
- [Talking-heads Attention](https://arxiv.org/abs/2003.02436)
- [Don't Stop Pretraining: Adapt Language Models to domains and tasks](https://www.aclweb.org/anthology/2020.acl-main.740.pdf)
